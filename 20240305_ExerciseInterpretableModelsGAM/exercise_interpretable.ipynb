{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interpretable Modelling of Credit Risk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">*Rakeen Rouf*\n",
        ">\n",
        ">*Bárbara Flores*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0DykLbMZ07x"
      },
      "source": [
        "\n",
        "As detailed in Cynthia Rudin's excellent commentary on interpretability [(ArXiV version here)](https://arxiv.org/abs/1811.10154), there are a plethora of reasons to avoid the use of black box models when models are being used to make high stakes decisions to may have life-altering effects on real people. Efforts to develop \"explainable black box models,\" while appealing for their potential to let us continuing using the same tools we always have and to creation explanations after the fact, are inherently flawed. As Rudin notes in my single favorite passage from her paper:\n",
        "\n",
        "> Explainable ML methods provide explanations that are not faithful to what the original model computes. Explanations must be wrong. They cannot have perfect fidelity with respect to the original model. If the explanation was completely faithful to what the original model computes, the explanation would equal the original model, and one would not need the original model in the first place, only the explanation. (In other words, this is a case where the original model would be interpretable.) This leads to the danger that any explanation method for a black box model can be an inaccurate representation of the original model in parts of the feature space.\n",
        ">\n",
        "> An inaccurate (low-fidelity) explanation model limits trust in the explanation, and by extension, trust in the black box that it is trying to explain. An explainable model that has a 90% agreement with the original model indeed explains the original model most of the time. However, an explanation model that is correct 90% of the time is wrong 10% of the time. If a tenth of the explanations are incorrect, one cannot trust the explanations, and thus one cannot trust the original black box. If we cannot know for certain whether our explanation is correct, we cannot know whether to trust either the explanation or the original model.\n",
        "\n",
        "With this motivation in mind, in this exercise, we will use a cutting edge interpretable modeling framework to model credit risk using data from the [14th Pacific-Asia Knowledge Discovery and Data Mining conference (PAKDD 2010)](https://pakdd.org/archive/pakdd2010/). This data covers the period of 2006 to 2009, and \"comes from a private label credit card operation of a Brazilian credit company and its partner shops.\" (The competition was won by [TIMi](https://timi.eu/blog/news/timi-top-winner-at-the-pakdd-2010-cup/), who purely by coincidence helped me complete my PhD dissertation research!).\n",
        "\n",
        "We will be working with Generalized Additive Models (GAMs) (not to be confused with Generalized *Linear* Models (GLMs) — GLMs are a special case of GAMs). In particular, we will be using the [pyGAM](https://pygam.readthedocs.io/en/latest/notebooks/tour_of_pygam.html), though this is far from the only GAM implementation out there. [mvgam](https://nicholasjclark.github.io/mvgam/) in R is probably considered the gold standard, as it was developed by a pioneering researcher of GAMs. `statsmodels` also has [an implementation](https://www.statsmodels.org/stable/gam.html), and GAM is also hiding in plain sight behind many other tools, like Meta's [Prophet](https://facebook.github.io/prophet/) time series forecasting library (which is GAM-based)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pygam import LogisticGAM, s, f, te\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from mpl_toolkits import mplot3d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Prep\n",
        "\n",
        "### Exercise 1\n",
        "\n",
        "The PADD 2010 data is in [this repository](https://github.com/nickeubank/MIDS_Data/tree/master/PAKDD%202010). You can find column names in `PAKDD2010_VariablesList.XLS` and the actual data in `PAKDD2010_Modeling_Data.txt`.\n",
        "\n",
        "Note: you may run into a string-encoding issue loading the `PAKDD2010_Modeling_Data.txt` data. All I'll say is that most latin-based languages used `latin8` as a text encoding prior to broad adoption of UTF-8. (Don't know about UTF? [Check out this video](https://www.youtube.com/watch?v=MijmeoH9LT4)!)\n",
        "\n",
        "Load the data (including column names)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID_CLIENT</th>\n",
              "      <th>CLERK_TYPE</th>\n",
              "      <th>PAYMENT_DAY</th>\n",
              "      <th>APPLICATION_SUBMISSION_TYPE</th>\n",
              "      <th>QUANT_ADDITIONAL_CARDS</th>\n",
              "      <th>POSTAL_ADDRESS_TYPE</th>\n",
              "      <th>SEX</th>\n",
              "      <th>MARITAL_STATUS</th>\n",
              "      <th>QUANT_DEPENDANTS</th>\n",
              "      <th>EDUCATION_LEVEL</th>\n",
              "      <th>...</th>\n",
              "      <th>FLAG_HOME_ADDRESS_DOCUMENT</th>\n",
              "      <th>FLAG_RG</th>\n",
              "      <th>FLAG_CPF</th>\n",
              "      <th>FLAG_INCOME_PROOF</th>\n",
              "      <th>PRODUCT</th>\n",
              "      <th>FLAG_ACSP_RECORD</th>\n",
              "      <th>AGE</th>\n",
              "      <th>RESIDENCIAL_ZIP_3</th>\n",
              "      <th>PROFESSIONAL_ZIP_3</th>\n",
              "      <th>TARGET_LABEL_BAD=1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "      <td>5</td>\n",
              "      <td>Web</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>F</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>N</td>\n",
              "      <td>32</td>\n",
              "      <td>595</td>\n",
              "      <td>595</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>C</td>\n",
              "      <td>15</td>\n",
              "      <td>Carga</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>F</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>N</td>\n",
              "      <td>34</td>\n",
              "      <td>230</td>\n",
              "      <td>230</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>C</td>\n",
              "      <td>5</td>\n",
              "      <td>Web</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>F</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>N</td>\n",
              "      <td>27</td>\n",
              "      <td>591</td>\n",
              "      <td>591</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>C</td>\n",
              "      <td>20</td>\n",
              "      <td>Web</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>F</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>N</td>\n",
              "      <td>61</td>\n",
              "      <td>545</td>\n",
              "      <td>545</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>C</td>\n",
              "      <td>10</td>\n",
              "      <td>Web</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>M</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>N</td>\n",
              "      <td>48</td>\n",
              "      <td>235</td>\n",
              "      <td>235</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 54 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID_CLIENT CLERK_TYPE  PAYMENT_DAY APPLICATION_SUBMISSION_TYPE  \\\n",
              "0          1          C            5                         Web   \n",
              "1          2          C           15                       Carga   \n",
              "2          3          C            5                         Web   \n",
              "3          4          C           20                         Web   \n",
              "4          5          C           10                         Web   \n",
              "\n",
              "   QUANT_ADDITIONAL_CARDS  POSTAL_ADDRESS_TYPE SEX  MARITAL_STATUS  \\\n",
              "0                       0                    1   F               6   \n",
              "1                       0                    1   F               2   \n",
              "2                       0                    1   F               2   \n",
              "3                       0                    1   F               2   \n",
              "4                       0                    1   M               2   \n",
              "\n",
              "   QUANT_DEPENDANTS  EDUCATION_LEVEL  ... FLAG_HOME_ADDRESS_DOCUMENT FLAG_RG  \\\n",
              "0                 1                0  ...                          0       0   \n",
              "1                 0                0  ...                          0       0   \n",
              "2                 0                0  ...                          0       0   \n",
              "3                 0                0  ...                          0       0   \n",
              "4                 0                0  ...                          0       0   \n",
              "\n",
              "   FLAG_CPF FLAG_INCOME_PROOF PRODUCT FLAG_ACSP_RECORD AGE RESIDENCIAL_ZIP_3  \\\n",
              "0         0                 0       1                N  32               595   \n",
              "1         0                 0       1                N  34               230   \n",
              "2         0                 0       1                N  27               591   \n",
              "3         0                 0       1                N  61               545   \n",
              "4         0                 0       1                N  48               235   \n",
              "\n",
              "   PROFESSIONAL_ZIP_3  TARGET_LABEL_BAD=1  \n",
              "0                 595                   1  \n",
              "1                 230                   1  \n",
              "2                 591                   0  \n",
              "3                 545                   0  \n",
              "4                 235                   1  \n",
              "\n",
              "[5 rows x 54 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "VariablesList = [\n",
        "    \"ID_CLIENT\",\n",
        "    \"CLERK_TYPE\",\n",
        "    \"PAYMENT_DAY\",\n",
        "    \"APPLICATION_SUBMISSION_TYPE\",\n",
        "    \"QUANT_ADDITIONAL_CARDS\",\n",
        "    \"POSTAL_ADDRESS_TYPE\",\n",
        "    \"SEX\",\n",
        "    \"MARITAL_STATUS\",\n",
        "    \"QUANT_DEPENDANTS\",\n",
        "    \"EDUCATION_LEVEL\",\n",
        "    \"STATE_OF_BIRTH\",\n",
        "    \"CITY_OF_BIRTH\",\n",
        "    \"NACIONALITY\",\n",
        "    \"RESIDENCIAL_STATE\",\n",
        "    \"RESIDENCIAL_CITY\",\n",
        "    \"RESIDENCIAL_BOROUGH\",\n",
        "    \"FLAG_RESIDENCIAL_PHONE\",\n",
        "    \"RESIDENCIAL_PHONE_AREA_CODE\",\n",
        "    \"RESIDENCE_TYPE\",\n",
        "    \"MONTHS_IN_RESIDENCE\",\n",
        "    \"FLAG_MOBILE_PHONE\",\n",
        "    \"FLAG_EMAIL\",\n",
        "    \"PERSONAL_MONTHLY_INCOME\",\n",
        "    \"OTHER_INCOMES\",\n",
        "    \"FLAG_VISA\",\n",
        "    \"FLAG_MASTERCARD\",\n",
        "    \"FLAG_DINERS\",\n",
        "    \"FLAG_AMERICAN_EXPRESS\",\n",
        "    \"FLAG_OTHER_CARDS\",\n",
        "    \"QUANT_BANKING_ACCOUNTS\",\n",
        "    \"QUANT_SPECIAL_BANKING_ACCOUNTS\",\n",
        "    \"PERSONAL_ASSETS_VALUE\",\n",
        "    \"QUANT_CARS\",\n",
        "    \"COMPANY\",\n",
        "    \"PROFESSIONAL_STATE\",\n",
        "    \"PROFESSIONAL_CITY\",\n",
        "    \"PROFESSIONAL_BOROUGH\",\n",
        "    \"FLAG_PROFESSIONAL_PHONE\",\n",
        "    \"PROFESSIONAL_PHONE_AREA_CODE\",\n",
        "    \"MONTHS_IN_THE_JOB\",\n",
        "    \"PROFESSION_CODE\",\n",
        "    \"OCCUPATION_TYPE\",\n",
        "    \"MATE_PROFESSION_CODE\",\n",
        "    \"MATE_EDUCATION_LEVEL\",\n",
        "    \"FLAG_HOME_ADDRESS_DOCUMENT\",\n",
        "    \"FLAG_RG\",\n",
        "    \"FLAG_CPF\",\n",
        "    \"FLAG_INCOME_PROOF\",\n",
        "    \"PRODUCT\",\n",
        "    \"FLAG_ACSP_RECORD\",\n",
        "    \"AGE\",\n",
        "    \"RESIDENCIAL_ZIP_3\",\n",
        "    \"PROFESSIONAL_ZIP_3\",\n",
        "    \"TARGET_LABEL_BAD=1\",\n",
        "]\n",
        "\n",
        "Modeling_Data = pd.read_csv(\n",
        "    \"https://media.githubusercontent.com/media/nickeubank/MIDS_Data/master/PAKDD%202010/PAKDD2010_Modeling_Data.txt\",\n",
        "    sep=\"\\t\",\n",
        "    header=None,\n",
        "    names=VariablesList,\n",
        "    index_col=None,\n",
        "    encoding=\"latin8\",\n",
        "    low_memory=False,\n",
        ")\n",
        "\n",
        "Modeling_Data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2\n",
        "\n",
        "There are a few variables with a lot of missing values (more than half missing). Given the limited documentation for this data it's a little hard to be sure why, but given the effect on sample size and what variables are missing, let's go ahead and drop them. You you end up dropping 6 variables.\n",
        "\n",
        "Hint: Some variables have missing values that aren't immediately obviously.\n",
        "\n",
        "(This is not strictly necessary at this stage, given we'll be doing more feature selection down the line, but keeps things easier knowing we don't have to worry about missingness later.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 54)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Modeling_Data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> First, we validate which variables have more than 25,000 records encoded as null. We can observe in the following summary that the variables `PROFESSIONAL_CITY`, `PROFESSIONAL_BOROUGH`, `MATE_PROFESSION_CODE`, and `MATE_EDUCATION_LEVEL` meet this condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PROFESSIONAL_CITY                 33783\n",
              "PROFESSIONAL_BOROUGH              33783\n",
              "MATE_EDUCATION_LEVEL              32338\n",
              "MATE_PROFESSION_CODE              28884\n",
              "PROFESSION_CODE                    7756\n",
              "OCCUPATION_TYPE                    7313\n",
              "MONTHS_IN_RESIDENCE                3777\n",
              "RESIDENCE_TYPE                     1349\n",
              "PROFESSIONAL_PHONE_AREA_CODE          0\n",
              "QUANT_SPECIAL_BANKING_ACCOUNTS        0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Modeling_Data.isnull().sum().sort_values(ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Later, upon visual inspection of the database, we noticed that the variables `EDUCATION_LEVEL`, `APPLICATION_SUBMISSION_TYPE`, `PROFESSIONAL_STATE`, and `PROFESSIONAL_PHONE_AREA_CODE` also appear to be problematic, which we will validate next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EDUCATION_LEVEL\n",
              "0    50000\n",
              "dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Modeling_Data.groupby(\"EDUCATION_LEVEL\").size().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "APPLICATION_SUBMISSION_TYPE\n",
              "Web      28206\n",
              "0        19461\n",
              "Carga     2333\n",
              "dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Modeling_Data.groupby(\n",
        "    \"APPLICATION_SUBMISSION_TYPE\").size().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PROFESSIONAL_PHONE_AREA_CODE\n",
              "       36532\n",
              "5       1457\n",
              "54      1109\n",
              "107      981\n",
              "97       644\n",
              "dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Modeling_Data.groupby(\"PROFESSIONAL_PHONE_AREA_CODE\").size().sort_values(\n",
        "    ascending=False\n",
        ").head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PROFESSIONAL_STATE\n",
              "      34307\n",
              "SP     2400\n",
              "RS     2092\n",
              "CE     1420\n",
              "BA     1387\n",
              "dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Modeling_Data.groupby(\"PROFESSIONAL_STATE\").size(\n",
        ").sort_values(ascending=False).head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Out of the 4 previously mentioned problematic variables, we observe that `PROFESSIONAL_STATE` and `PROFESSIONAL_PHONE_AREA_CODE` have more than 25,000 missing values. Additionally, `EDUCATION_LEVEL` presents the value 0 for all its data, while the encoding in the dictionary starts from 1. \n",
        ">\n",
        ">Therefore, our final list of variables to be removed, which have more than half missing values, includes `EDUCATION_LEVEL`, `PROFESSIONAL_CITY`, `PROFESSIONAL_BOROUGH`, `MATE_PROFESSION_CODE`, `MATE_EDUCATION_LEVEL`, `PROFESSIONAL_STATE`, and `PROFESSIONAL_PHONE_AREA_CODE`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_to_drop = [\n",
        "    \"EDUCATION_LEVEL\",\n",
        "    \"PROFESSIONAL_CITY\",\n",
        "    \"PROFESSIONAL_BOROUGH\",\n",
        "    \"MATE_PROFESSION_CODE\",\n",
        "    \"MATE_EDUCATION_LEVEL\",\n",
        "    \"PROFESSIONAL_STATE\",\n",
        "    \"PROFESSIONAL_PHONE_AREA_CODE\",\n",
        "]\n",
        "\n",
        "Modeling_Data = Modeling_Data.drop(columns_to_drop, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3\n",
        "\n",
        "Let's start off by fitting a model that uses the following variables:\n",
        "\n",
        "```\n",
        "\"QUANT_DEPENDANTS\",\n",
        "\"QUANT_CARS\",\n",
        "\"MONTHS_IN_RESIDENCE\",\n",
        "\"PERSONAL_MONTHLY_INCOME\",\n",
        "\"QUANT_BANKING_ACCOUNTS\",\n",
        "\"AGE\",\n",
        "\"SEX\",\n",
        "\"MARITAL_STATUS\",\n",
        "\"OCCUPATION_TYPE\",\n",
        "\"RESIDENCE_TYPE\",\n",
        "\"RESIDENCIAL_STATE\",\n",
        "\"RESIDENCIAL_CITY\",\n",
        "\"RESIDENCIAL_BOROUGH\",\n",
        "\"RESIDENCIAL_ZIP_3\"\n",
        "```\n",
        "\n",
        "(GAMs don't have any automatic feature selection methods, so these are based on my own sense of features that are likely to matter. A fully analysis would entail a few passes at feature refinement)\n",
        "\n",
        "Plot and otherwise characterize the distributions of all the variables we may use. If you see anything bananas, adjust how terms enter your model. Yes, pyGAM has flexible functional forms, but giving the model features that are engineered to be more substantively meaningful (e.g., taking log of income) will aid model estimation. \n",
        "\n",
        "You should probably do something about the functional form of *at least* `PERSONAL_MONTHLY_INCOME`, and `QUANT_DEPENDANTS`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "Modeling_Data = Modeling_Data[\n",
        "    [\n",
        "        \"QUANT_DEPENDANTS\",\n",
        "        \"QUANT_CARS\",\n",
        "        \"MONTHS_IN_RESIDENCE\",\n",
        "        \"PERSONAL_MONTHLY_INCOME\",\n",
        "        \"QUANT_BANKING_ACCOUNTS\",\n",
        "        \"AGE\",\n",
        "        \"SEX\",\n",
        "        \"MARITAL_STATUS\",\n",
        "        \"OCCUPATION_TYPE\",\n",
        "        \"RESIDENCE_TYPE\",\n",
        "        \"RESIDENCIAL_STATE\",\n",
        "        \"RESIDENCIAL_CITY\",\n",
        "        \"RESIDENCIAL_BOROUGH\",\n",
        "        \"RESIDENCIAL_ZIP_3\",\n",
        "        \"TARGET_LABEL_BAD=1\",\n",
        "    ]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">First, given the wide range of values taken by the variable `PERSONAL_MONTHLY_INCOME` and the fact that the variance increases as the level of the variable grows, we transform this variable by taking the logarithm, resulting in the creation of the variable `LOG_PERSONAL_MONTHLY_INCOME`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PERSONAL_MONTHLY_INCOME\n",
        "Modeling_Data[\"LOG_PERSONAL_MONTHLY_INCOME\"] = np.log(\n",
        "    Modeling_Data[\"PERSONAL_MONTHLY_INCOME\"]\n",
        ")\n",
        "\n",
        "Modeling_Data.drop(columns=[\"PERSONAL_MONTHLY_INCOME\"], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Second, we can observe that in the variable `QUANT_DEPENDANTS`, there is an outlier with a value of 53. Since there is only 1 record with this value, we have no problem in removing it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QUANT_DEPENDANTS\n",
              "0     33655\n",
              "1      7004\n",
              "2      5363\n",
              "3      2465\n",
              "4       860\n",
              "5       365\n",
              "6       139\n",
              "7        65\n",
              "8        35\n",
              "9        14\n",
              "10       13\n",
              "11        7\n",
              "12        6\n",
              "13        4\n",
              "14        3\n",
              "15        1\n",
              "53        1\n",
              "dtype: int64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# QUANT_DEPENDANTS\n",
        "Modeling_Data.groupby(\"QUANT_DEPENDANTS\").size().sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "Modeling_Data = Modeling_Data[Modeling_Data[\"QUANT_DEPENDANTS\"] != 53]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">After the transformations mentioned earlier, we can observe the distribution of our numerical variables in the following histograms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_num = [\n",
        "    \"QUANT_DEPENDANTS\",\n",
        "    \"QUANT_CARS\",\n",
        "    \"MONTHS_IN_RESIDENCE\",\n",
        "    \"LOG_PERSONAL_MONTHLY_INCOME\",\n",
        "    \"QUANT_BANKING_ACCOUNTS\",\n",
        "    \"AGE\",\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "for i, variable in enumerate(variables_num, start=1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    sns.histplot(data=Modeling_Data[variable], bins=20)\n",
        "    plt.title(f\"\\nFrequency of {variable}\\n\")\n",
        "    plt.xlabel(\"Value\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Similarly, we can observe the distribution of our categorical variables in the following bar plots.\n",
        ">\n",
        ">It is worth mentioning that some of these variables may not be as useful in their interpretation, as we find that the variables `MARITAL_STATUS`, `OCCUPATION_TYPE`, and `RESIDENCE_TYPE` are labeled as \"Encoding not informed\" in our dictionary. While we may encounter a model that informs us whether these variables are important or not, we will not be able to determine their true contribution to the model due to the lack of information about their encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_cat = [\n",
        "    \"SEX\",\n",
        "    \"MARITAL_STATUS\",\n",
        "    \"OCCUPATION_TYPE\",\n",
        "    \"RESIDENCE_TYPE\",\n",
        "    \"RESIDENCIAL_STATE\",\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "for i, variable in enumerate(variables_cat, start=1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    sns.countplot(data=Modeling_Data, x=variable)\n",
        "    plt.title(f\"\\nFrequency of variable '{variables_cat[i - 1]}'\\n\")\n",
        "    plt.xticks(fontsize=8)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Aditionally, the categorical variables `RESIDENCIAL_CITY`, `RESIDENCIAL_BOROUGH`, and `RESIDENCIAL_ZIP_3` cannot be plotted because they have 3,529, 14,511, and 794 different unique categories respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\n",
        "    f\"Number of unique categories in RESIDENCIAL_CITY: {Modeling_Data['RESIDENCIAL_CITY'].nunique():,}\"\n",
        ")\n",
        "print(\n",
        "    f\"Number of unique categories in RESIDENCIAL_BOROUGH: {Modeling_Data['RESIDENCIAL_BOROUGH'].nunique():,}\"\n",
        ")\n",
        "print(\n",
        "    f\"Number of unique categories in RESIDENCIAL_ZIP_3: {Modeling_Data['RESIDENCIAL_ZIP_3'].nunique():,}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Finally, for the variables `SEX`, `MARITAL_STATUS`, `OCCUPATION_TYPE`, and `RESIDENCE_TYPE`, we encounter values that are not within the valid elements in our dictionary. For example, \"N\" in the case of `SEX` and 0 in the case of `MARITAL_STATUS`, `OCCUPATION_TYPE`, and `RESIDENCE_TYPE`. Therefore, we will transform these values to null."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Modeling_Data[Modeling_Data[\"SEX\"] == \"N\"] = np.nan\n",
        "Modeling_Data[Modeling_Data[\"SEX\"] == \" \"] = np.nan\n",
        "Modeling_Data[Modeling_Data[\"MARITAL_STATUS\"] == 0] = np.nan\n",
        "Modeling_Data[Modeling_Data[\"OCCUPATION_TYPE\"] == 0] = np.nan\n",
        "Modeling_Data[Modeling_Data[\"RESIDENCE_TYPE\"] == 0] = np.nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4\n",
        "\n",
        "Geographic segregation means residency data often contains LOTS of information. But there's a problem with `RESIDENCIAL_CITY` and `RESIDENCIAL_BOROUGH`. What is the problem?\n",
        "\n",
        "In any real project, this would be something absolutely worth resolving, but for this exercise, we'll just drop all three string `RESIDENCIAL_` variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Modeling_Data[\"RESIDENCIAL_CITY\"].value_counts().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Modeling_Data[\"RESIDENCIAL_BOROUGH\"].value_counts(\n",
        ").sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Added to the previously mentioned fact that we have many different categories for the same variable, we can see that in the case of `RESIDENCIAL_CITY` and `RESIDENCIAL_BOROUGH`, we also encounter city names written in many different ways, which may include differences in capitalization, abbreviations, etc.\n",
        ">\n",
        ">Although we could address these differences with the purpose of achieving uniformity, for the purposes of this exercise, we will simply remove these variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Modeling_Data.drop(\n",
        "    [\"RESIDENCIAL_CITY\", \"RESIDENCIAL_BOROUGH\", \"RESIDENCIAL_STATE\"],\n",
        "    axis=1,\n",
        "    inplace=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8pKcxeLZ6xx"
      },
      "source": [
        "## Model Fitting\n",
        "\n",
        "### Exercise 5\n",
        "\n",
        "First, use `train_test_split` to do an 80/20 split of your data. Then, using the `TARGET_LABEL_BAD` variable, fit a classification model on this data. Optimize with `gridsearch`. Use splines for continuous variables and factors for categoricals.\n",
        "\n",
        "At this point we'd *ideally* be working with 11 variables. However pyGAM can get a little slow with factor features with lots of values + lots of unique values (e.g., 50,000 observations and the *many* values of `RESIDENCIAL_ZIP` takes about 15 minutes on my computer). In that configuration, you should get a model fit in 10-15 seconds.\n",
        "\n",
        "So let's start by fitting a model that also excludes `RESIDENCIAL_ZIP`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "one_hot_encoded = pd.get_dummies(\n",
        "    Modeling_Data[\"SEX\"], prefix=\"SEX\", drop_first=True)\n",
        "Modeling_Data = pd.concat([Modeling_Data, one_hot_encoded], axis=1)\n",
        "Modeling_Data.drop(\"SEX\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Modeling_Data.dropna(inplace=True)\n",
        "\n",
        "X = Modeling_Data[\n",
        "    [\n",
        "        \"QUANT_DEPENDANTS\",\n",
        "        \"QUANT_CARS\",\n",
        "        \"MONTHS_IN_RESIDENCE\",\n",
        "        \"LOG_PERSONAL_MONTHLY_INCOME\",\n",
        "        \"QUANT_BANKING_ACCOUNTS\",\n",
        "        \"AGE\",\n",
        "        \"SEX_M\",\n",
        "        \"MARITAL_STATUS\",\n",
        "        \"OCCUPATION_TYPE\",\n",
        "        \"RESIDENCE_TYPE\",\n",
        "    ]\n",
        "]\n",
        "\n",
        "X = X.copy()\n",
        "\n",
        "# recoding MARITAL_STATUS to include 0, to help with partial dependence plots\n",
        "X.loc[:, \"MARITAL_STATUS\"] = X[\"MARITAL_STATUS\"] - 1\n",
        "# recoding Residence Type to include 0, to help with partial dependence plots\n",
        "X.loc[:, \"RESIDENCE_TYPE\"] = X[\"RESIDENCE_TYPE\"] - 1\n",
        "# recoding Occupation Type to include 0, to help with partial dependence plots\n",
        "X.loc[:, \"OCCUPATION_TYPE\"] = X[\"OCCUPATION_TYPE\"] - 1\n",
        "\n",
        "y = Modeling_Data[[\"TARGET_LABEL_BAD=1\"]]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=123\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gam = LogisticGAM(s(0) + s(1) + s(2) + s(3) + s(4) +\n",
        "                  s(5) + f(6) + f(7) + f(8) + f(9))\n",
        "gam.gridsearch(X_train.values, y_train.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 6\n",
        "\n",
        "Create a (naive) confusion matrix using the predicted values you get with `predict()` on your test data. Our stakeholder cares about two things: \n",
        "\n",
        "- maximizing the number of people to whom they extend credit, and\n",
        "- the false negative rate (the share of people identified as \"safe bets\" who aren't, and who thus default).\n",
        "\n",
        "How many \"good bets\" does the model predict (true negatives), and what is the [False Omission Rate](https://en.wikipedia.org/wiki/False_omission_rate) (the share of predicted negatives that are false negatives)?\n",
        "\n",
        "Looking at the confusion matrix, how did the model maximize accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> If we calculate our predictions in a \"naive\" manner, meaning without adjusting the predefined parameters of the model, we obtain these results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = gam.predict(X_test.values)\n",
        "conf_matrix = confusion_matrix(y_test.values, y_pred)\n",
        "conf_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "true_negative, false_positive, false_negative, true_positive = conf_matrix.ravel()\n",
        "print(f\"TN: {true_negative:,}\")\n",
        "print(f\"FP: {false_positive:,}\")\n",
        "print(f\"FN: {false_negative:,}\")\n",
        "print(f\"TP: {true_positive:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> With our confusion matrix information, we can deduce the following:\n",
        ">\n",
        "> - The total number of individuals to whom credit is extended would be TN + FN = `7,398`.\n",
        ">- However, among these individuals to whom credit would be extended, it is expected that `1,882` would be false negatives. In other words, these are individuals identified as \"safe bets\" who actually default.\n",
        "> - Therefore, the proportion of individuals identified as \"safe bets\" who actually default would be $\\frac{FN}{TN + FN} = \\frac{1,882}{7,398}$ = `0.254`.\n",
        "> \n",
        "> - The \"good bets\" predicted by the model (true negatives) are `5,516`.\n",
        "> - The False Omission Rate or the share of predicted negatives that are false negatives would be  $\\frac{FN}{FN + TN} = \\frac{1,882}{1,882 + 5,516}$ = `0.254`. \n",
        ">\n",
        ">When looking at the confusion matrix, it appears that the model is prioritizing accuracy optimization by minimizing false positives, as evidenced by the low count of only 1 false positive. However, in this case, we are likely not employing the best strategy, as the model is allowing for a high number of false negatives.\n",
        ">\n",
        ">Minimizing false positives would result in minimizing the denial of credit to good payers. However, by tolerating a high number of false negatives, the model is allowing a large number of bad payers that are being mistakenly identified as good and are being approved.\n",
        ">\n",
        ">A more balanced strategy could involve adjusting the model's classification thresholds to allow for greater sensitivity in detecting bad payers, which would help reduce the number of false negatives without compromising too much on the accuracy of identifying good payers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 7\n",
        "\n",
        "Suppose your stakeholder wants to minimize false negative rates. How low of a [False Omission Rate](https://en.wikipedia.org/wiki/False_omission_rate) (the share of predicted negatives that are false negatives) can you get (assuming more than, say, 10 true negatives), and how many \"good bets\" (true negatives) do they get at that risk level?\n",
        "\n",
        "Hint: use `predict_proba()`\n",
        "\n",
        "Note: One *can* use class weights to shift the emphasis of the original model fitting, but for the moment let's just play with `predict_proba()` and thresholds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Our objective is to determine a threshold that maximizes False Negative Rates while ensuring that the number of True negatives exceeds 10.\n",
        ">\n",
        ">We proceed to compute our metrics for 100 different thresholds ranging from 0 to 1. The metrics we will take into consideration are those that our stakeholder is interested in, namely TN, FNR, and FOR.\n",
        ">\n",
        ">We have the following formulas:\n",
        ">\n",
        ">$FNR = \\frac{FN}{FN + TP}$\n",
        ">\n",
        ">$FOR = \\frac{FN}{FN + TN}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_proba = gam.predict_proba(X_test.values)\n",
        "thresholds = np.linspace(0, 1, 100)\n",
        "\n",
        "FNR_list = []\n",
        "FOR_list = []\n",
        "TN_list = []\n",
        "thresholds_list = []\n",
        "\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred = (y_pred_proba > threshold).astype(int)\n",
        "    conf_matrix = confusion_matrix(y_test.values, y_pred)\n",
        "    true_negative, false_positive, false_negative, true_positive = conf_matrix.ravel()\n",
        "\n",
        "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "        FNR = false_negative / (false_negative + true_positive)\n",
        "        FOR = false_negative / (false_negative + true_negative)\n",
        "\n",
        "    # We will only save in the list the values that meet our constraint of TN > 10.\n",
        "    if true_negative > 10:\n",
        "        FNR_list.append(FNR)\n",
        "        FOR_list.append(FOR)\n",
        "        TN_list.append(true_negative)\n",
        "        thresholds_list.append(threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Then we proceed to graph these variables for different thresholds, with the objective of finding an optimum according to our stakeholder's requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax1 = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "ax1.plot(thresholds_list, FNR_list, label=\"FNR\", color=\"blue\")\n",
        "ax1.plot(thresholds_list, FOR_list, label=\"FOR\", color=\"red\")\n",
        "ax1.set_xlabel(\"Thresholds\")\n",
        "ax1.set_ylabel(\"FNR and FOR\", color=\"black\")\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(thresholds_list, TN_list, label=\"TN\", color=\"green\")\n",
        "ax2.set_ylabel(\"Number of people\", color=\"green\")\n",
        "\n",
        "lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
        "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc=\"center right\")\n",
        "\n",
        "plt.title(\n",
        "    \"False Negative Rate, False Omission Rate, and True Negative\\n vs. Thresholds (With True Negative > 10)\"\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> The previous graph was built using the results of our model for different thresholds, with the constraint of having a total of true negatives greater than 10. \n",
        ">\n",
        "> As the threshold increases, we notice a simultaneous increase in our False Negative Rate, which eventually reaches 100%, and in our true negatives, which also increase until they cover 100% of the non-risky individuals in the dataset, approximately 55,000. The False Omission Rate grows until stabilizing at 0.25.\n",
        ">\n",
        ">Our lowest False Negative Rate occurs with a threshold of approximately `0.15`, resulting in:\n",
        ">\n",
        ">False Negative Rate: 0.00212\n",
        ">\n",
        ">False Omission Rate: 0.11429\n",
        ">\n",
        ">True Negatives = 31 > 10.\n",
        ">\n",
        ">However, this model predicts a very small number of true negatives, specifically, 31 customers who would be eligible for a loan according to our stakeholder's criteria and who indeed turn out to be good customers. \n",
        ">\n",
        "> While we meet the constraints specified in the problem statement, the restricted number of eligible customers might not generate profitability for the stakeholder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = (y_pred_proba > 0.15).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test.values, y_pred)\n",
        "\n",
        "true_negative, false_positive, false_negative, true_positive = conf_matrix.ravel()\n",
        "print(f\"TN: {true_negative:,}\")\n",
        "print(f\"FP: {false_positive:,}\")\n",
        "print(f\"FN: {false_negative:,}\")\n",
        "print(f\"TP: {true_positive:,}\")\n",
        "print()\n",
        "\n",
        "FNR = false_negative / (false_negative + true_positive)\n",
        "FOR = false_negative / (false_negative + true_negative)\n",
        "print(\"False Negative Rate:\", round(FNR, 5))\n",
        "print(\"False Omission Rate:\", round(FOR, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 8\n",
        "\n",
        "If the stakeholder wants to maximize true negatives and can tolerate a false omission rate of 19%, how many true negatives will they be able to enroll?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> We will use the same strategy as in the previous exercise. We will calculate our relevant metrics for different threshold values, considering the restriction that the false omission rate should be less than 19%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FNR_list = []\n",
        "FOR_list = []\n",
        "TN_list = []\n",
        "thresholds_list = []\n",
        "\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred = (y_pred_proba > threshold).astype(int)\n",
        "    conf_matrix = confusion_matrix(y_test.values, y_pred)\n",
        "    true_negative, false_positive, false_negative, true_positive = conf_matrix.ravel()\n",
        "\n",
        "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "        FNR = false_negative / (false_negative + true_positive)\n",
        "        FOR = false_negative / (false_negative + true_negative)\n",
        "\n",
        "    # We will only save in the list the values that meet our constraint of false omission rate <= 19%,.\n",
        "    if FOR <= 0.19:\n",
        "        FNR_list.append(FNR)\n",
        "        FOR_list.append(FOR)\n",
        "        TN_list.append(true_negative)\n",
        "        thresholds_list.append(threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Then we proceed to graph how these values behave for different threshold values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax1 = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "ax1.plot(thresholds_list, FNR_list, label=\"FNR\", color=\"blue\")\n",
        "ax1.plot(thresholds_list, FOR_list, label=\"FOR\", color=\"red\")\n",
        "ax1.set_xlabel(\"Thresholds\")\n",
        "ax1.set_ylabel(\"FNR and FOR\", color=\"black\")\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(thresholds_list, TN_list, label=\"TN\", color=\"green\")\n",
        "ax2.set_ylabel(\"Number of people\", color=\"green\")\n",
        "\n",
        "lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
        "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc=\"center right\")\n",
        "\n",
        "plt.title(\n",
        "    \"False Negative Rate, False Omission Rate, and True Negative\\n vs. Thresholds (With FOR <= 19%)\"\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> We can see in the previous graph that our maximum value of true negatives, considering our restriction, is achieved for a threshold of around 0.24. For this threshold, our maximum value of true negatives is 2,334.\n",
        ">\n",
        "> It's worth mentioning that the tradeoff in this situation compared to the previous case is that we increase our False Negatives, and consequently our False Negative Rate and False Omission Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = (y_pred_proba > 0.24).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test.values, y_pred)\n",
        "\n",
        "true_negative, false_positive, false_negative, true_positive = conf_matrix.ravel()\n",
        "print(f\"TN: {true_negative:,}\")\n",
        "print(f\"FP: {false_positive:,}\")\n",
        "print(f\"FN: {false_negative:,}\")\n",
        "print(f\"TP: {true_positive:,}\")\n",
        "print()\n",
        "\n",
        "FNR = false_negative / (false_negative + true_positive)\n",
        "FOR = false_negative / (false_negative + true_negative)\n",
        "print(\"False Negative Rate:\", round(FNR, 5))\n",
        "print(\"False Omission Rate:\", round(FOR, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's See This Interpretability!\n",
        "\n",
        "We're using GAMs for their interpretability, so let's use it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbexO4ZqZ9bd"
      },
      "source": [
        "### Exercise 9\n",
        "\n",
        "\n",
        "Plot the partial dependence plots for all your continuous factors with 95% confidence intervals (I have three, at this stage).\n",
        "\n",
        "If you get an error like this when generating `partial_dependence` errors:\n",
        "\n",
        "```python\n",
        "----> pdep, confi = gam.partial_dependence(term=i, X=XX, width=0.95)\n",
        "\n",
        "...\n",
        "ValueError: X data is out of domain for categorical feature 4. Expected data on [1.0, 2.0], but found data on [0.0, 0.0]\n",
        "```\n",
        "\n",
        "it's because you have a variable set as a factor that doesn't have values of `0`. pyGAM is assuming `0` is the excluded category. Just recode the variable to ensure 0 is used to identify one of the categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lets look at the model summary to find the three significant continuous variables\n",
        "gam.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "important_features = [2, 3, 5]\n",
        "\n",
        "# Plotting the partial dependence plots\n",
        "plt.figure()\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
        "titles = [\"Months in Residence\", \"LOG_PERSONAL_MONTHLY_INCOME\", \"Age\"]\n",
        "for i, ax in enumerate(axs):\n",
        "    title = titles[i]\n",
        "    i = important_features[i]\n",
        "    XX = gam.generate_X_grid(term=i)\n",
        "    ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))\n",
        "    ax.plot(\n",
        "        XX[:, i], gam.partial_dependence(term=i, X=XX, width=0.95)[1], c=\"r\", ls=\"--\"\n",
        "    )\n",
        "    # adding x and y labels\n",
        "    ax.set_xlabel(title)\n",
        "    ax.set_ylabel(\"Partial Dependence\")\n",
        "    ax.set_title(title)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 10\n",
        "\n",
        "How does the partial correlation with respect to age look?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> With respect to age, it looks like as age increases, initially that leads to less Bad Loans per unit increase in age (signified by the negative gradient). But as age increases beyond ~65, that leads to more Bad loans as age increases (signified by positive gradient). Therefore, there seems to be two zones of correlation, one positve, one negative. Overall, \"age\" looks to have a non-linear relationship with \"Bad_Loan\", with the weakest correlation atound 65 yers of age."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 11\n",
        "\n",
        "Refit your model, but this time impose [monotonicity or concavity/convexity](https://pygam.readthedocs.io/en/latest/notebooks/tour_of_pygam.html#Penalties-/-Constraints) on the relationship between age and credit risk (which makes more sense to you?). Fit the model and plot the new partial dependence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Considering the partial correlation observed with respect to age in the previous graph, we believe that `the relationship between age and credit risk that makes the most sense to us is a convex relationship`. Therefore, we impose this convexity constraint and refit the model below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gam = LogisticGAM(\n",
        "    s(0)\n",
        "    + s(1)\n",
        "    + s(2)\n",
        "    + s(3)\n",
        "    + s(4)\n",
        "    + s(5, constraints=\"convex\")  # convex constraint\n",
        "    + f(6)\n",
        "    + f(7)\n",
        "    + f(8)\n",
        "    + f(9)\n",
        ")\n",
        "gam.gridsearch(X_train.values, y_train.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "important_features = [2, 3, 5]\n",
        "\n",
        "# Plotting the partial dependence plots\n",
        "plt.figure()\n",
        "title = \"Age\"\n",
        "i = 5\n",
        "XX = gam.generate_X_grid(term=i)\n",
        "plt.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))\n",
        "plt.plot(XX[:, i], gam.partial_dependence(\n",
        "    term=i, X=XX, width=0.95)[1], c=\"r\", ls=\"--\")\n",
        "plt.title(title)\n",
        "# adding x and y labels\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Partial Dependence\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 12\n",
        "\n",
        "Functional form constraints are often about fairness or meeting regulatory requirements, but they can also prevent overfitting.\n",
        "\n",
        "Does this change the number of \"true negatives\" you can enroll below a false omission rate of 19%?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_proba = gam.predict_proba(X_test.values)\n",
        "\n",
        "FNR_list = []\n",
        "FOR_list = []\n",
        "TN_list = []\n",
        "thresholds_list = []\n",
        "\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred = (y_pred_proba > threshold).astype(int)\n",
        "    conf_matrix = confusion_matrix(y_test.values, y_pred)\n",
        "    true_negative, false_positive, false_negative, true_positive = conf_matrix.ravel()\n",
        "\n",
        "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "        FNR = false_negative / (false_negative + true_positive)\n",
        "        FOR = false_negative / (false_negative + true_negative)\n",
        "\n",
        "    # We will only save in the list the values that meet our constraint of false omission rate <= 19%,.\n",
        "    if FOR <= 0.19:\n",
        "        FNR_list.append(FNR)\n",
        "        FOR_list.append(FOR)\n",
        "        TN_list.append(true_negative)\n",
        "        thresholds_list.append(threshold)\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "ax1.plot(thresholds_list, FNR_list, label=\"FNR\", color=\"blue\")\n",
        "ax1.plot(thresholds_list, FOR_list, label=\"FOR\", color=\"red\")\n",
        "ax1.set_xlabel(\"Thresholds\")\n",
        "ax1.set_ylabel(\"FNR and FOR\", color=\"black\")\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(thresholds_list, TN_list, label=\"TN\", color=\"green\")\n",
        "ax2.set_ylabel(\"Number of people\", color=\"green\")\n",
        "\n",
        "lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
        "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc=\"center right\")\n",
        "\n",
        "plt.title(\n",
        "    \"False Negative Rate, False Omission Rate, and True Negative\\n vs. Thresholds (With FOR <= 19%)\"\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = (y_pred_proba > 0.23).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test.values, y_pred)\n",
        "\n",
        "true_negative, false_positive, false_negative, true_positive = conf_matrix.ravel()\n",
        "print(f\"TN: {true_negative:,}\")\n",
        "print(f\"FP: {false_positive:,}\")\n",
        "print(f\"FN: {false_negative:,}\")\n",
        "print(f\"TP: {true_positive:,}\")\n",
        "print()\n",
        "\n",
        "FNR = false_negative / (false_negative + true_positive)\n",
        "FOR = false_negative / (false_negative + true_negative)\n",
        "print(\"False Negative Rate:\", round(FNR, 5))\n",
        "print(\"False Omission Rate:\", round(FOR, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> We can see in the previous graph that our maximum value of true negatives, considering our restriction, is achieved for a threshold of around 0.23. For this threshold, our maximum value of true negatives is 2,089, which is less than the one from the previous implementation of the model without the monotonicity constraints (2,334). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 13\n",
        "\n",
        "In the preceding exercises, we allowed pyGAM to choose its own smoothing parameters / coefficient penalties. This makes life easy, but it isn't always optimal, especially because when it does so, it picks the same smoothing penalty (the `lambda` in `.summary()`) for all terms.\n",
        "\n",
        "(If you haven't seen them let, penalities are designed to limit overfitting by, basically, \"penalizing\" big coefficients on different terms. This tends to push models towards smoother fits.)\n",
        "\n",
        "[To get around this, we can do a grid or random search.](https://pygam.readthedocs.io/en/latest/notebooks/quick_start.html#Automatically-tune-the-model) This is definitely a little slow, but let's give it a try!\n",
        "\n",
        "Then following the model given in the docs linked above, let's do a random search. Make sure your initial random points has a shape of `100 x (the number of terms in your model)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gam.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lams = np.random.rand(100, 10)\n",
        "lams = lams * 6 - 3  # shift values to -3, 3\n",
        "lams = 10**lams  # transforms values to 1e-3, 1e3\n",
        "\n",
        "gam = LogisticGAM(\n",
        "    s(0)\n",
        "    + s(1)\n",
        "    + s(2)\n",
        "    + s(3)\n",
        "    + s(4)\n",
        "    + s(5, constraints=\"convex\")  # convex constraint\n",
        "    + f(6)\n",
        "    + f(7)\n",
        "    + f(8)\n",
        "    + f(9)\n",
        ")\n",
        "gam.gridsearch(X_train.values, y_train.values, lam=lams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gam.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">In the above summary table, we can clearly notice how the grid search arrived at a different lambda value for ewach feature. This is likely to help against overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 14\n",
        "\n",
        "How many true negatives can you get now at a less than 19% False Omission Rate?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_proba = gam.predict_proba(X_test.values)\n",
        "\n",
        "FNR_list = []\n",
        "FOR_list = []\n",
        "TN_list = []\n",
        "thresholds_list = []\n",
        "\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred = (y_pred_proba > threshold).astype(int)\n",
        "    conf_matrix = confusion_matrix(y_test.values, y_pred)\n",
        "    true_negative, false_positive, false_negative, true_positive = conf_matrix.ravel()\n",
        "\n",
        "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "        FNR = false_negative / (false_negative + true_positive)\n",
        "        FOR = false_negative / (false_negative + true_negative)\n",
        "\n",
        "    # We will only save in the list the values that meet our constraint of false omission rate <= 19%,.\n",
        "    if FOR <= 0.19:\n",
        "        FNR_list.append(FNR)\n",
        "        FOR_list.append(FOR)\n",
        "        TN_list.append(true_negative)\n",
        "        thresholds_list.append(threshold)\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "ax1.plot(thresholds_list, FNR_list, label=\"FNR\", color=\"blue\")\n",
        "ax1.plot(thresholds_list, FOR_list, label=\"FOR\", color=\"red\")\n",
        "ax1.set_xlabel(\"Thresholds\")\n",
        "ax1.set_ylabel(\"FNR and FOR\", color=\"black\")\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(thresholds_list, TN_list, label=\"TN\", color=\"green\")\n",
        "ax2.set_ylabel(\"Number of people\", color=\"green\")\n",
        "\n",
        "lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
        "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc=\"center right\")\n",
        "\n",
        "plt.title(\n",
        "    \"False Negative Rate, False Omission Rate, and True Negative\\n vs. Thresholds (With FOR <= 19%)\"\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = (y_pred_proba > 0.23).astype(int)\n",
        "conf_matrix = confusion_matrix(y_test.values, y_pred)\n",
        "\n",
        "true_negative, false_positive, false_negative, true_positive = conf_matrix.ravel()\n",
        "print(f\"TN: {true_negative:,}\")\n",
        "print(f\"FP: {false_positive:,}\")\n",
        "print(f\"FN: {false_negative:,}\")\n",
        "print(f\"TP: {true_positive:,}\")\n",
        "print()\n",
        "\n",
        "FNR = false_negative / (false_negative + true_positive)\n",
        "FOR = false_negative / (false_negative + true_negative)\n",
        "print(\"False Negative Rate:\", round(FNR, 5))\n",
        "print(\"False Omission Rate:\", round(FOR, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> We can see in the previous graph that our maximum value of true negatives, considering our restriction, is achieved for a threshold of around 0.23. For this threshold, our maximum value of true negatives is 2,104. Which is  slightly more than the model without the custom LAMS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 15\n",
        "Add an interaction term between age and personal income."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lams = np.random.rand(100, 10)\n",
        "lams = lams * 6 - 3  # shift values to -3, 3\n",
        "lams = 10**lams  # transforms values to 1e-3, 1e3\n",
        "\n",
        "gam = LogisticGAM(\n",
        "    s(0)\n",
        "    + s(1)\n",
        "    + s(2)\n",
        "    + s(4)\n",
        "    + te(5, 3, constraints=\"convex\")  # convex constraint\n",
        "    + f(6)\n",
        "    + f(7)\n",
        "    + f(8)\n",
        "    + f(9)\n",
        ")\n",
        "gam.gridsearch(X_train.values, y_train.values, lam=lams)\n",
        "\n",
        "# lets look at our models summary\n",
        "gam.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 16\n",
        "\n",
        "Now visualize the [partial dependence interaction term.](https://pygam.readthedocs.io/en/latest/notebooks/tour_of_pygam.html#Terms-and-Interactions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
        "XX = gam.generate_X_grid(term=10, meshgrid=True)\n",
        "Z = gam.partial_dependence(term=4, X=XX, meshgrid=True)\n",
        "\n",
        "ax = plt.axes(projection=\"3d\")\n",
        "ax.plot_surface(XX[0], XX[1], Z, cmap=\"viridis\")\n",
        "\n",
        "ax.set_xlabel(\"AGE\")\n",
        "ax.set_ylabel(\"LOG_PERSONAL_MONTHLY_INCOME\")\n",
        "ax.set_zlabel(\"Partial Dependence\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> The 3D graph above shows that the relationship between age, income, and credit risk is complex and is best understood in the context of how these variables affect each other. In general, we can see that, on the one hand, credit risk tends to decrease as LOG_PERSONAL_MONTHLY_INCOME increases. However, the relationship between credit risk and age is convex. At younger ages, credit risk decreases with age, but after reaching the age of 60, credit risk tends to increase with each additional year"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 17\n",
        "\n",
        "Finally, another popular interpretable model is the `ExplainableBoostingClassifier`. You can learn more [about it here](https://interpret.ml/docs/ebm.html), though how much sense it will make to you may be limited if you aren't familiar with gradient boosting yet. Still, at least one of your classmates prefers it to pyGAM, so give it a try using this code:\n",
        "\n",
        "\n",
        "```python\n",
        "from interpret.glassbox import ExplainableBoostingClassifier\n",
        "from interpret import show\n",
        "import warnings\n",
        "\n",
        "ebm = ExplainableBoostingClassifier()\n",
        "ebm.fit(X_train, y_train)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "   warnings.simplefilter(\"ignore\")\n",
        "\n",
        "   ebm_global = ebm.explain_global()\n",
        "   show(ebm_global)\n",
        "\n",
        "   ebm_local = ebm.explain_local(X_train, y_train)\n",
        "   show(ebm_local)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from interpret.glassbox import ExplainableBoostingClassifier\n",
        "from interpret import show\n",
        "import warnings\n",
        "\n",
        "ebm = ExplainableBoostingClassifier()\n",
        "ebm.fit(X_train, y_train)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "\n",
        "    ebm_global = ebm.explain_global()\n",
        "    show(ebm_global)\n",
        "\n",
        "    ebm_local = ebm.explain_local(X_train, y_train)\n",
        "    show(ebm_local)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">From the first figure, we can confirm that AGE is a very important factor in credit default. Overall, we can see the importance of demographic and financial features in credit risk assesent. The model appeaks to capture both individual and interactive effects of the features. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
